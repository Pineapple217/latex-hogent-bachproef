%---------- Inleiding ---------------------------------------------------------

% TODO: Is dit voorstel gebaseerd op een paper van Research Methods die je
% vorig jaar hebt ingediend? Heb je daarbij eventueel samengewerkt met een
% andere student?
% Zo ja, haal dan de tekst hieronder uit commentaar en pas aan.

%\paragraph{Opmerking}

% Dit voorstel is gebaseerd op het onderzoeksvoorstel dat werd geschreven in het
% kader van het vak Research Methods dat ik (vorig/dit) academiejaar heb
% uitgewerkt (met medesturent VOORNAAM NAAM als mede-auteur).
% 

\section{Inleiding}%
\label{sec:inleiding}

In een tijdperk waar real-time data essentieel is voor het nemen van beslissingen, is webscraping een steeds belangrijker instrument geworden. Traditioneel worden webscraping-taken vaak gesynchroniseerd door tijdschema's of events van de databank. Event broker gebaseerde systemen kunnen potentieel deze taken op een flexibelere manier automatiseren. Een event broker zoals Apache Kafka of RabbitMQ kan gebruikt worden om webscraping te triggeren op basis van specifieke gebeurtenissen, wat kan leiden tot efficiëntere en schaalbare systemen. Dit onderzoek gaat de mogelijkheden van event-driven architecturen na voor webscraping, de verschillen ervan in vergelijking met traditionele methoden, de uitdagingen op het gebied van fouttolerantie en consistentie. Er word ook na gegaan welke invloed het heeft op de resulteerde data en zijn consistentie, en welke type trigger kunnen gebruikt worden.

%---------- Stand van zaken ---------------------------------------------------

\section{Literatuurstudie}%
\label{sec:literatuurstudie}

% Voor literatuurverwijzingen zijn er twee belangrijke commando's:
% \autocite{KEY} => (Auteur, jaartal) Gebruik dit als de naam van de auteur
%   geen onderdeel is van de zin.
% \textcite{KEY} => Auteur (jaartal)  Gebruik dit als de auteursnaam wel een
%   functie heeft in de zin (bv. ``Uit onderzoek door Doll & Hill (1954) bleek
%   ...'')

Data speelt in onze huidige maatschappij een rol die niet te negeren is \autocite{lohr2012age}. In vrijwel alle sectoren wordt data ingezet om processen te verbeteren, inzicht te krijgen in gedrag en toekomstvoorspellingen te doen. Bij gevolg daar van is het verzamel van data zeer belangrijk. Data kan op vele manieren worden verkregen, maak bedrijven geven hun data niet zomaar op een computer leesbare manier vrij. Webscraping is een gevestigde techniek voor het verzamelen van deze soort online data. Vaak word worden scripts op een bepaald tijdsinterval uitgevoerd en de resulterende data weggeschreven naar een databank \autocite{zhao2022web}. Maar door de groeiende schaal dat webscraping plaats vind word het tijd voor vernieuwde architecturen die beter kunnen omgaan met de uitdagingen van grootschalige dataverzameling en de complexe infrastructuur die daarbij komt kijken \autocite{khder2021web}.

Event-driven architecturen (EDA) met event-hubs zoals bijvoorbeeld Apache Kafka, bieden een dynamischer alternatief. In plaats van tijdgebaseerde triggers, activeren deze systemen scraping-taken op basis van gebeurtenissen zoals pagina-updates of gebruikersacties, wat leidt tot efficiëntere dataverzameling \autocite{coronado2015context}. Dit komt omdat EDA het makkelijk maakt om de uitvoer van een scraper een andere scraper laat starten. Dit concept staat centraal bij een event-driven architectuur \autocite{michelson2006event}.

Event-hubs die gebruikt worden als centerpunt van event-driven architecturen brengen ook nog hun eigen voordelen. Systemen zoals Apache Kafka zijn namelijk ontworpen voor gedistribueerde taken en hoge weerstand tegen dataverlies \autocite{garg2013apache}. Dit komt ze vaak bestaan uit clusters van meerdere fysieke of virtuele server. Dit brengt wel extra complexiteit met zich mee maar hoe groter de schaal van de webscraping hoe meer de voordelen van event-hubs zullen uitblinken \autocite{vyas2022performance}. Desondanks mogen de nadelen niet genegeerd worden. Door clustering van de event-hub kan enkel eventual consistency gegarandeerd worden. Dit is in tegenstelling tot klassieke SQL-databanken die contante consistency garanderen. Gelukkig heeft dit feit weinig impact op de werking van webscrapers. Wat nogmaals aantoont data EDA zeer voordelig is voor deze soort taken.


%---------- Methodologie ------------------------------------------------------
\section{Methodologie}%
\label{sec:methodologie}

De methodologie van deze studie richt zich op de ontwikkeling van een proof-of-concept (POC) voor een event broker-georiënteerd webscraping systeem. Dit systeem wordt gebouwd om webscraping-taken te automatiseren op basis van verschillende soorten triggers, zoals tijdsintervallen, pagina-updates en gebruikersinteracties. De gekozen event broker voor dit systeem is Apache Kafka, dat verantwoordelijk zal zijn voor het ontvangen en distribueren van events die scraping-taken starten, maar ook het verwerken van de webscraping data zelf in real-time. De POC zal geschreven worden in Python vanwege het strake webscraping ecosysteem en integratie met andere tools.

De eerste stap in de opzet van het systeem is het definiëren van de triggers. Dit omvat zowel tijdgebaseerde triggers (bijvoorbeeld scraping elke 30 minuten) als gebeurtenisgebaseerde triggers (bijvoorbeeld wanneer een website een update publiceert). Elke trigger zal een event genereren dat door Kafka wordt verwerkt en naar de juiste scraper wordt gestuurd. Verschillende soorten scrapers worden ingesteld, waarbij elk verantwoordelijk is voor een specifiek domein of type data.

Vervolgens wordt het systeem geconfigureerd om schaling en fouttolerantie te testen. Scrapers worden als microservices geïmplementeerd, zodat ze afzonderlijk kunnen worden opgeschaald op basis van de belasting. Hierbij wordt onderzocht hoe het systeem reageert op verhoogde verkeersbelasting, het parallel draaien van meerdere scrapers en het herstel van fouten, zoals netwerkproblemen of mislukte scraping-pogingen.

% De laatste fase omvat het monitoren en meten van de prestaties (Moet SMART zijn, mss van scope droppen) van het POC-systeem, met nadruk op nauwkeurigheid en efficiëntie van de scraping-processen onder verschillende omstandigheden. Door te experimenteren met verschillende triggers en workloads, kan worden geëvalueerd welke triggers het beste werken voor specifieke scenario's en hoe het systeem zich gedraagt in termen van snelheid, betrouwbaarheid en foutafhandeling.

%---------- Verwachte resultaten ----------------------------------------------
\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

Het onderzoek verwacht dat event broker-gestuurde architecturen de webscraping efficiënter maken, vooral in scenario's waar meerdere triggers, zoals pagina-updates of gebruikersinteracties, van belang zijn. Het is waarschijnlijk dat event-driven systemen schaalbaarder zijn dan traditionele methoden, vooral omdat ze minder afhankelijk zijn van rigide schema's en beter kunnen omgaan met real-time data. Er wordt ook verwacht dat event brokers bijdragen aan een robuustere fouttolerantie en consistente uitvoering van scraping-taken, doordat ze de taken kunnen verdelen en beheren via een gedecentraliseerd systeem. Er zullen echter uitdagingen zijn op het gebied van consistentie en systeemcomplexiteit.

