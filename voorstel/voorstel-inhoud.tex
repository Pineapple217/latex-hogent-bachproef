%---------- Inleiding ---------------------------------------------------------

% TODO: Is dit voorstel gebaseerd op een paper van Research Methods die je
% vorig jaar hebt ingediend? Heb je daarbij eventueel samengewerkt met een
% andere student?
% Zo ja, haal dan de tekst hieronder uit commentaar en pas aan.

%\paragraph{Opmerking}

% Dit voorstel is gebaseerd op het onderzoeksvoorstel dat werd geschreven in het
% kader van het vak Research Methods dat ik (vorig/dit) academiejaar heb
% uitgewerkt (met medesturent VOORNAAM NAAM als mede-auteur).
% 

\section{Inleiding}%
\label{sec:inleiding}

In een tijdperk waar real-time data essentieel is voor het nemen van beslissingen, is webscraping een steeds belangrijker instrument geworden. Traditioneel worden webscraping-taken vaak gesynchroniseerd door tijdschema's of events van de databank. Event broker gebaseerde systemen kunnen potentieel deze taken op een flexibelere manier automatiseren. Een event broker zoals Apache Kafka of RabbitMQ kan gebruikt worden om webscraping te triggeren op basis van specifieke gebeurtenissen, wat kan leiden tot efficiëntere en schaalbare systemen. Dit onderzoek gaat de mogelijkheden van event-driven architecturen na voor webscraping en de verschillen ervan in vergelijking met traditionele methoden. Meer specifiek: wat zijn de uitdagingen op het gebied van fouttolerantie en consistentie? Welke invloed heeft het op de resulterende data en zijn consistentie? Welke type triggers kunnen gebruikt worden en welke hiervan leveren optimale resultaten? Verder zullen vragen over de implementatie en de POC beantwoord worden, namelijk: Welke technieken kunnen worden gebruikt om real-time wijzigingen op webpagina's te detecteren als triggers voor webscraping? Welke architectuurelementen zijn nodig om een event-driven webscraping-systeem te realiseren dat dynamische triggers ondersteunt? Hoe kunnen event brokers zoals Apache Kafka specifiek bijdragen aan het verbeteren van schaalbaarheid en fouttolerantie bij webscraping-taken?

%---------- Stand van zaken ---------------------------------------------------

\section{Literatuurstudie}%
\label{sec:literatuurstudie}

% Voor literatuurverwijzingen zijn er twee belangrijke commando's:
% \autocite{KEY} => (Auteur, jaartal) Gebruik dit als de naam van de auteur
%   geen onderdeel is van de zin.
% \textcite{KEY} => Auteur (jaartal)  Gebruik dit als de auteursnaam wel een
%   functie heeft in de zin (bv. ``Uit onderzoek door Doll & Hill (1954) bleek
%   ...'')

Data speelt in onze huidige maatschappij een rol die niet te negeren is \autocite{lohr2012age}. In vrijwel alle sectoren wordt data ingezet om processen te verbeteren, inzicht te krijgen in gedrag en toekomstvoorspellingen te doen. Bij gevolg daar van is het verzamel van data zeer belangrijk. Data kan op vele manieren worden verkregen, maak bedrijven geven hun data niet zomaar op een computer leesbare manier vrij. Webscraping is een gevestigde techniek voor het verzamelen van deze soort online data. Vaak word worden scripts op een bepaald tijdsinterval uitgevoerd en de resulterende data weggeschreven naar een databank \autocite{zhao2022web}. Maar door de groeiende schaal dat webscraping plaats vind word het tijd voor vernieuwde architecturen die beter kunnen omgaan met de uitdagingen van grootschalige dataverzameling en de complexe infrastructuur die daarbij komt kijken \autocite{khder2021web}.

Event-driven architecturen (EDA) met event-hubs zoals bijvoorbeeld Apache Kafka, bieden een dynamischer alternatief. In plaats van tijdgebaseerde triggers, activeren deze systemen scraping-taken op basis van gebeurtenissen zoals pagina-updates of gebruikersacties, wat leidt tot efficiëntere dataverzameling \autocite{coronado2015context}. Dit komt omdat EDA het makkelijk maakt om de uitvoer van een scraper een andere scraper laat starten. Dit concept staat centraal bij een event-driven architectuur \autocite{michelson2006event}.

Event-hubs die gebruikt worden als centerpunt van event-driven architecturen brengen ook nog hun eigen voordelen. Systemen zoals Apache Kafka zijn namelijk ontworpen voor gedistribueerde taken en hoge weerstand tegen dataverlies \autocite{garg2013apache}. Dit komt ze vaak bestaan uit clusters van meerdere fysieke of virtuele server. Dit brengt wel extra complexiteit met zich mee maar hoe groter de schaal van de webscraping hoe meer de voordelen van event-hubs zullen uitblinken \autocite{vyas2022performance}. Desondanks mogen de nadelen niet genegeerd worden. Door clustering van de event-hub kan enkel eventual consistency gegarandeerd worden. Dit is in tegenstelling tot klassieke SQL-databanken die contante consistency garanderen. Gelukkig heeft dit feit weinig impact op de werking van webscrapers. Wat nogmaals aantoont data EDA zeer voordelig is voor deze soort taken.


%---------- Methodologie ------------------------------------------------------
\section{Methodologie}%
\label{sec:methodologie}

De methodologie van deze studie richt zich op de ontwikkeling van een proof-of-concept (POC) voor een event broker-georiënteerd webscraping systeem. Dit systeem wordt gebouwd om webscraping taken te automatiseren op basis van verschillende soorten triggers, zoals tijdsintervallen, pagina updates en gebruikersinteracties. De gekozen event broker voor dit systeem is Apache Kafka, dat verantwoordelijk zal zijn voor het ontvangen en distribueren van events die scraping-taken starten, maar ook het verwerken van de webscraping data zelf in real-time. De POC zal geschreven worden in Python vanwege het strake webscraping ecosysteem en integratie met andere tools.

Kafka is voor deze POC gekozen omdat het het meest gebruikte event hub is de standaard is in veel industrieën. Brokers zoals Kafka zijn van de grond af ontworpen en gemaakt om schaalbaar en fouttolerant te zijn. Kafka (en vele anderen) gebruikt een vorm van sharding om dit te bereiken. Omdat het systeem uit meerdere nodes bestaat (ook wel individuele brokers genoemd) kan het het aantal van deze nodes makkelijk veranderen zonder het systeem te beïnvloeden.

Voor data serialisatie zal simpel weg JSON gebruikt worden om de complexiteit lager te houden. Maar event hubs zoals Kafka hebben geen voorkeur van data type. Er kunnen schemas zoals Protocol Buffers of Apache Avro voor consistente date formaten. In grote systemen is dit sterk aan te raden maar voor de scoop concreet te houden zal hier JSON gebruikt worden.

De eerste stap in de opzet van het systeem is het definiëren van de triggers. Dit omvat zowel tijdgebaseerde triggers (bijvoorbeeld scraping elke 30 minuten) als gebeurtenisgebaseerde triggers (bijvoorbeeld wanneer een website een update publiceert). Elke trigger zal een event genereren dat door Kafka wordt verwerkt en naar de juiste scraper wordt gestuurd. Verschillende soorten scrapers worden ingesteld, waarbij elk verantwoordelijk is voor een specifiek domein of type data.

Het detecteren van pagina updates voor het genereren van deze events zal gebeuren door herhaaldelijk de websites te scrapen en deze data te pushen als een event. Een andere service kan dan deze events uitlezen en verschillen detecteren. Deze verschillen kunnen opnieuw gepusht worden als een nieuw event, op de event hub. Deze aanpak zorgt er voor dat alle state centraal bij de event hub bewaard blijft. Dit heeft als gevolg dat scraper nodes kunnen op starten en afsluiten zonder enige vorm van data verlies of inconsistentie.

Voor deze consistentie te behouden is het belangrijk dat altijd gebruikt word gemaakt van de juiste type transacties en dat de state in een scraper node minimaal blijft. Dit word voornamelijk gestuurd door de hiervoor besproken architectuur maar het blijft belangrijk dat de individuele services zich hier aan blijven houden om consistentie te garanderen. Dit kan een uitdaging zijn in grotere teams maar door de juiste architectuur consistente toe te passen zullen veel problemen hier vermeden worden.

Vervolgens wordt het systeem geconfigureerd om schaling en fouttolerantie te testen. Scrapers worden als microservices geïmplementeerd, zodat ze afzonderlijk kunnen worden opgeschaald op basis van de belasting. Hierbij wordt onderzocht hoe het systeem reageert op verhoogde verkeersbelasting, het parallel draaien van meerdere scrapers en het herstel van fouten, zoals netwerkproblemen of mislukte scraping-pogingen. Voor de fouttolerantie en consistentie van het systeem te testen zullen er meerdere nodes van zowel de event broker als van de scrapers gedraaid worden. Ook zul het falen van deze nodes gesimuleerd worden voor de reactie van het gehele  systeem te testen. Voor een productie omgeven te simuleren zal er gebruikt worden gemaakt van Docker en meerdere containers.

% De laatste fase omvat het monitoren en meten van de prestaties (Moet SMART zijn, mss van scope droppen) van het POC-systeem, met nadruk op nauwkeurigheid en efficiëntie van de scraping-processen onder verschillende omstandigheden. Door te experimenteren met verschillende triggers en workloads, kan worden geëvalueerd welke triggers het beste werken voor specifieke scenario's en hoe het systeem zich gedraagt in termen van snelheid, betrouwbaarheid en foutafhandeling.

%---------- Verwachte resultaten ----------------------------------------------
\section{Verwacht resultaat, conclusie}%
\label{sec:verwachte_resultaten}

Het onderzoek verwacht dat event broker-gestuurde architecturen de webscraping efficiënter maken, vooral in scenario's waar meerdere triggers, zoals pagina-updates of gebruikersinteracties, van belang zijn. Het is waarschijnlijk dat event-driven systemen schaalbaarder zijn dan traditionele methoden, vooral omdat ze minder afhankelijk zijn van rigide schema's en beter kunnen omgaan met real-time data. Er wordt ook verwacht dat event brokers bijdragen aan een robuustere fouttolerantie en consistente uitvoering van scraping-taken, doordat ze de taken kunnen verdelen en beheren via een gedecentraliseerd systeem. Dit zal kunnen worden aangetoond het virtuele testomgeving, gevormd met de hulp van Docker. Er zullen echter uitdagingen zijn op het gebied van consistentie en systeemcomplexiteit. Maar er word verwacht de gekozen architectuur die minimale hoeveelheid data op scraper nodes opslaat voor een robuuste consistentie zal zorgen.

